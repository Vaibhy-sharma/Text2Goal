INPUT EMBEDDING :- CONVERTS THE INPUT TOKENS TO VECTORS OF FIXED DIMENSION 
USING LEARNED EMBEDDINGS.

POSITIONAL ENCODING :- HAVING ADDITIONAL VECTORS TO DESCRIBE THE POSITION OF 
A TOKEN IN A SEQUENCE.

MULTI-HEAD ATTENTION : IN THIS LAYER EACH WORD IS MAPPED TO ALL OTHER WORDS 
TO DETERMINE HOW IT FITS IN A SEQUENCE. 
IT HAS EIGHT LAYERS TO DETERMINE DIFFERENT CONTEXTS OF EACH WORD. 

MASKED MULTI-HEAD LAYER : IT'S SAME AS MULTI-HEAD ATTENTION LAYER EXCEPT THAT THE FUTURE 
WORDS OF TARGET SENTENCES ARE HIDDEN FROM THE LAYER. 
SO IT APPLIES ATTENTION MECHANISM TILL CURRENT WORD OF THE OUTPUT WORD VECTOR ONLY.    






